# Compound Poisson Processes {#chap-cpp}

In this chapter, we consider a stochastic process $X(t)$ which is not necessarily a counting process. We say $X(t)$ is a *compound Poisson process* if 
\[
X(t) = \sum_{i = 1}^{N(t)} Y_i
\]
where $N(t)$ is a Poisson process with rate $\lambda$ and $Y_1, \ldots$ are independent, identically distributed random variables, which are also independent of $N(t)$.

::: {#exm-compoundprocess1}

## BSC enterings

If you let $X(t)$ denote the number of people who enter Busch Student Center at or before time $t$, then this may not be a Poisson process. If we count all of the people who enter as a group as entering at the same time, then we could possibly model it as a compound Poisson process, where the arrival times of groups is a Poisson process, and the number of people in the group is a separate random variable (that we would need to model).

:::

Other examples given in the text include the number of people involved in car accidents at a certain intersection, the amount of money that visitors to a casino lose, and number of people entering a movie theater. 

We can compute the mean and variance of a compound Poisson process as follows:

::: {#thm-meanvariancecompoundprocess}

## Mean and Variance of Compound Poisson Process

The mean $E[X(t)] = \lambda t E[Y]$ and the variance is ${\mathrm{Var}}(X(t)) = \lambda t E[Y^2]$.
:::

Let's look at a simulation of a compound Poisson process in order to check these results via an example. We assume that $N(t)$ is a Poisson process with rate 2, and $Y_i$ are iid uniform random variables on the interval $[0, 1]$. From Theorem @thm-meanvariancecompoundprocess, we see that 
\[
E[X(3)] = 2 \times 3 \times \frac 12 = 3
\]
and 
\[
{\mathrm{Var}}(X(t)) = 3 \times 2 \times \int_0^1 x^2\, dx = 2
\]
We check this via a simulation. Note that we don't need to know **when** the events occur exactly, only how many of them there are before time $t = 3$. That is given by `r`pois(1, 6)`, as below. 

```{r}
lambda <- 2
t <- 3
num_events <- rpois(1, lambda * t)
x_3 <- sum(runif(num_events)) #if num_events == 0, then this gives 0, which is what we want
```

That is how we simulate a single trial of the compound Poisson process. To do multiple trials, we put it inside of `replicate`.

```{r}
sim_data <- replicate(10000, {
  num_events <- rpois(1, lambda * t)
  x_3 <- sum(runif(num_events)) 
})
mean(sim_data)
var(sim_data)
```

If you run the above code a few times, you will see that the mean and variance of the compound Poisson process are as predicted by Theorem @thm-meanvariancecompoundprocess.

## Modeling a Compound Poisson Process

In this section, we are interested in the reverse problem: if we are given data, how do we estimate $\lambda$ and the distribution of $Y_1, \ldots, Y_{N(t)}$? For our purposes, we will make some assumptions about what kind of random variable $Y_1, \ldots, Y_n$ are and then use a goodness-of-fit test to see whether it is a reasonable fit to the data.

We start by modeling data that we **know** the true generative process of. This is so that we can see that out technique is working. In the next example, we will apply our technique to data that we don't know the true generative process for.

### Simulated data

We assume that $N(t)$ is Poisson with rate 2, and that $Y_1, \ldots, Y_{N(t)}$ are iid **zero-truncated** Poisson random variables with rate 3. Let's imagine that this is the number of people entering a movie theater. For our simulated data, we are going to assume that we have collected data from time 0 to time 4, and we noted the time of arrival and the number of We create our simulated data as follows. The arrival times $S_1, \ldots, S_{N(t)}$ say **when** the groups arrived to the theater, and the values $Y_1, \ldots, Y_{N(t)}$ say **how many** people were in each group. We use zero-truncated Poisson because we are assuming that each $Y_i > 0$. 

```{r}
lambda <- 2
t <- 4
num_events <- rpois(1, lambda * t)
time_of_events <- sort(runif(num_events, 0, t))
num_in_family <- actuar::rztpois(num_events, 3) #sampling from zero-truncated
data.frame(trial = 1,
           time = time_of_events,
           num_in_family = num_in_family)
```

To create a large sample of this type, we use `purrr::map_df`.

```{r}
set.seed(1)
sim_data <- purrr::map_df(1:300, function(x) {
  num_events <- rpois(1, lambda * t)
  time_of_events <- sort(runif(num_events, 0, t))
  num_in_family <- actuar::rztpois(num_events, 3) #sampling from zero-truncated
  data.frame(trial = x,
             time = time_of_events,
             num_in_family = num_in_family)
})
```

First things first, let's check whether every trial had at least one family arrive:

```{r}
length(unique(sim_data$trial)) #yep!
```

In order to estimate the rate of the Poisson process $N(t)$, we recall that the expected value of $N(4)$ is $4\lambda$. We can estimate $E[N(4)]$ from the data by counting the total number of occurrences by time 4, and dividing by the total number of trials. In other words, 

```{r}
nrow(sim_data)/300
```

Our estimate for trhe mean of $N(4)$ is `r round(nrow(sim_data)/300, 3)`, so our estimate for $\lambda$ is our previous estimate divided by 4; namely,  `r round(nrow(sim_data)/300/4, 3)`. You should compare that to the true value of 2.

Now we turn to estimating the mean of the truncated Poisson process. The method of moments estimator for $\lambda$, is given by 
\[
\frac {\hat \lambda}{1 - e^{-\hat \lambda}} = \overline{x}
\]
where $\overline{x}$ is the sample mean. We compute the sample mean as follows:

```{r}
mean(sim_data$num_in_family)
```

and to solve for $\hat \lambda$, well, that is a bit tricky, but we already know it is 3. Let's just check:

```{r}
3/(1 - exp(-3))
```

Pretty good. If we want to solve for $\hat \lambda$, we can use `optimize`:

```{r}
optimize(f = function(lambda) {(lambda/(1 - exp(-lambda)) - mean(sim_data$num_in_family))^2 },
         interval = c(0, 10))
```

Our estimate $\hat \lambda$ is given by the value in `$minimum`.  We can also check via a plot that it is about $\hat \lambda = 3$.

```{r}
curve(x/(1 - exp(-x)), from = 0, to = 5)
abline(h = mean(sim_data$num_in_family))
abline(v = 3, lty = 2, col = 2)
```


### Non-simulated data

We consider the airplane crash data from [Kaggle](https://www.kaggle.com/datasets/thedevastator/airplane-crashes-and-fatalities). I imagine that airplane crashes are **not** a homogeneous Poisson process over the time frame of this data set (1908-present), because, well, that would just be too much to believe! Safety has improved (surely) but also the number of flights has increased, so it is not at all clear what to make of it. We'll probably need to restrict down to a time interval where the rate of crashes is approximately constant. 

```{r}
#| warning: false
library(tidyverse)
dd <- read.csv("data/airplane_crashes.csv") %>% 
  janitor::clean_names()
dd <- as_tibble(dd)
dd
```

The first thing I want to do is create a single date/time variable that encodes the time of the crash. To do so, we use the R package `lubridate`. If a crash doesn't have an associated time, we just assume it happened at noon. I don't think that will make a difference, but we will need to make sure our bins are not too small, in which case it would make a difference. Alternatively, I could've deleted those, but that seems like a lot of data to delete (42 percent!). One stil fails to parse (index 3267) because there is a typo in the time (114:20). Let's assume they meant 14:20.

```{r}
mean(dd$time == "")
dd <- dd %>% 
  mutate(time = ifelse(time == "", "12:00", time)) %>% 
  mutate(time = ifelse(time == "114:20", "14:20", time)) %>% 
  mutate(date_time = lubridate::mdy_hm(paste0(date, time, sep = " ")))
```

We also want to restrict to fatal airplane crashes for the purposes of this exercise, as this data set is probably missing quite a few crashes that were not fatal.

```{r}
dd <- dd %>% 
  filter(fatalities > 0, aboard > 40)
```

OK, now let's get a histogram of the number of **accidents** over time.

```{r}
ggplot(dd, aes(x = date_time)) + 
  geom_histogram(bins = 100)
```

We can see that this is almost surely not a homogeneous Poisson process, but if we restrict to, say, 1980-2000 it might be. Let's check it out.

```{r}
aa <- dd %>% 
  filter(date_time > lubridate::mdy("12/31/1989") & date_time < lubridate::mdy("1/1/2001"))
ss <- hist(aa$date_time, breaks = 30)
```

When creating histograms, R sometimes tries to guess what the natural range of values is based on the observed range of values. For example, in this case, the histogram bins range from `r ss$breals[1]` to  `r ss$breaks[36]`, where the actual range of values (in seconds) is `r as.numeric(min(aa$date_time))` to `r as.numeric(max(aa$date_time))`. So, the first and the last bins are different sizes than the rest, and should have different probabilities associated with them. The histogram also **changes** the number of breaks to make a better looking plot. In this case, it made 36 bins!

::: callout-tryit
Confirm that a histogram using `hist(aa$date_time, breaks = 30)` has 36 bins. (It is the length of the counts variable that `hist` creates.) Confirm the values of the min and max break in the previous paragraph.
:::

To fix this, we provide explicit locations of breaks, in terms of seconds.

```{r}
mindt <- min(as.numeric(aa$date_time))
maxdt <- max(as.numeric(aa$date_time))
ss <- hist(as.numeric(aa$date_time), breaks = seq(mindt - 1, maxdt + 1, length.out = 37))
```

This histogram looks quite different, even though it is the same number of breaks as the previous histogram.

::: callout-warning
Let's check to see whether this passes a goodness-of-fit test for a homogeneous Poisson process. We estimate the rate in terms of accidents per year. Note that there are 11 years in the data. We need to be really careful with units here! The units in `breaks` are **seconds** and we computed the rate in terms of **years**! We also need to check the number of bins that `hist` makes; just because you tell it to do 30, for example, doesn't mean that there will be **exactly** 30 bins!!
:::

```{r}
rate <- 851/11 #rate in accidents per year
```

We now compute the expected and the observed number of accidents in each bin based on our estimate of the rate being `round(rate, 2)`. The variable `ss$counts` contains the observed values in the bins, so we just need to compute the expected value. There are roughly 31557600 seconds in a year, so we divide the rate in accidents per year by that number to get accidents per second.

```{r}
observed_accidents <- ss$counts
expected_accidents <- rate * (ss$breaks[2] - ss$breaks[1])/31557600 
test_stat <- sum((observed_accidents - expected_accidents)^2/expected_accidents)
pchisq(test_stat, df = length(ss$counts) - 1 - 1, lower.tail = F)
```

We fail to reject that airplane crashes are a homogeneous Poisson process. Let's continue now with **fatalities**. That is surely not a homogeneous Poisson process, because it violates the assumption that multiple occurrences cannot happen at the same time. Let's see what the distribution of fatalities looks like.

```{r}
aa <- aa %>% 
  mutate(survivors = aboard - fatalities)
hist(aa$survivors)
```

It is really right skew. Maybe Poisson? No, for Poisson the mean is equal to the variance and check out our data!

```{r}
muhat <- mean(aa$survivors, na.rm = T)
varhat <- var(aa$survivors, na.rm = T)
muhat
varhat
```

Many times when modeling a discrete rv with variance larger than the mean, we use **negative binomial** random variables. We choose the `size` and `mu` parameter to match the mean and variance of our data. The variance is `mu` + `mu^2`/`size` in this parametrization. Solving for `size` we get 

$$
\mu^2/(\widehat{\mathrm{Var}(x)} - \mu) = \mathrm{size}
$$

This is our estimate for the `size` parameter.

```{r}
sizehat <- muhat^2/(varhat - muhat)
sizehat
```

::: callout-warning

This cannot be correct! Negative binomial random variables usually can take on zero with high probability, while the number of fatalities in the plane crashes we observed were almost never zero. In subsequent notes, we will use a **zero truncated** negative binomial random variable, which also will not be a good fit, but at least it has a chance.

:::

This seems to be a reasonable model so far. $X(t)$ is a compound Poisson process with rate $\lambda = 77.36$ per year, and $Y_1, \ldots, Y_{N(t)}$ iid negative binomial random variables with size 0.47 and mean 31.6. We have already checked whether the occurrences are approximately a Poisson process, let's see how to check whether the total fatalities are approximately negative binomial with parameters estimated above. 

We first set up our bins for the number of fatalities. We'll set them up so that we have about the same number of expected outcomes in each bin.

```{r}
qnbinom(seq(0, 1, length.out = 20), size = sizehat, mu = muhat)
```

**Ruh-Roh!!!** We forgot that we restricted to crashes that had a positive number of fatalities! The most likely outcome for a negative binomial random variable is 0! I don't need to check whehter this is correct, it is obviously incorrect. Let's just look at the  observed and expected for small values of fatalities.

```{r}
table(aa$survivors)[1:10]
round(dnbinom(0:11, size = sizehat, mu = muhat) * 335)

probs <- c(dnbinom(0:30, size =sizehat, mu = muhat), 1 - sum(dnbinom(0:30, size = sizehat, mu = muhat)))
expected <- 1375 * probs
observed <- table(aa$survivors)
observed <- c(observed[1:31], sum(observed[32:119]))
observed
expected
teststat <- sum((expected - observed)^2/expected)
pchisq(teststat, df = 32 - 1 - 2, lower.tail = F)
(expected - observed)^2/expected
```

What we needed was a **zero-truncated** negative binomial random variable. This is similar to the zero-truncated Poisson above, and we just need to know how to estimate the parameters associated with it.  We see it has two parameters, size and prob, and the mean and variance are given in the help page. We need to find a combination of size and prob that leads to the observed mean and variance.

```{r}
mu <- function(r, p) {
  r*(1-p)/(p*(1-p^r)) 
}
vari <- function(r, p) {
  (r*(1-p)*(1-(1+r*(1-p))*p^r))/(p*(1-p^r))^2 
}

optim(par = list(r = .08, p= .012), fn = function(x) {
  r <- x[1]
  p <- x[2]
  (mu(r,p) - muhat)^2 + (vari(r,p) - varhat)^2
})

r <- .08016
p <- .012118
hist(actuar::rztnbinom(850, size = r, prob = p))
```

Now, we check whether the observed counts are close enough to the expected counts. The expected count falls below 5 when the number of fatalities is 30, so we bin together 30+. 

::: callout-tryit
You could also make more bins by binning together 30-40, 41-50, 51-60, 61-70, 71-80, 81-90, and 91+. This makes the R code a bit more challenging, but you should try to do it!
:::

```{r}
probs <- c(actuar::dztnbinom(1:30, r, p), 1 - sum(actuar::dztnbinom(1:30, r, p)))
expected <- 835 * probs
observed <- table(aa$fatalities)
observed <- c(observed[1:30], sum(observed[31:117]))
teststat <- sum((expected - observed)^2/expected)
pchisq(teststat, df = 31 - 1 - 3, lower.tail = F)
```

We reject the null hypothesis that a zero-truncated negative binomial random variable is a good model for this data. Darn it!

```{r}
hist(aa$aboard)
table(aa$aboard)
plot(density(aa$fatalities/aa$aboard, na.rm = T))
```


