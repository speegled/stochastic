# Hidden Markov Models

In this chapter, we will learn about simulating and training Hidden Markov Models (HMMs). The goal will be to apply HMMs to classification problems.

## Definition of HMM

A *hidden Markov model (HMM)* is a statistical Markov model in which the system being modeled is assumed to be a Markov process $X$ with unobservable states.[^wiki] There is, however, **something** that can be observed, and the distribution of that something depends on which (unobservable) state that the Markov process $X$ is in. More formally, a hidden Markov model requires two stochastic processes $X_n$ and $Y_n$. The process $X_n$ is a Markov process, whose states we cannot directly observe. The process $Y_n$ follows some distribution, which only depends on $X_n$; that is, it is independent of $X_1, \ldots, X_{n-1}, Y_1, \ldots, Y_{n - 1}$. We assume for simplicity that the possible values of $Y_n$ are the numbers $1, \ldots, M$ and the states of the Markov chain are $1, \ldots, N$. The *emission probabilities* or *output probabilities* are $P(Y_n = i|X_n = j)$.

::: {#exm-hmm} 
Examples of Hidden Markov Models

- Suppose there are three urns with different numbers of red and green balls in them. You start by randomly selecting one of the urns, drawing a ball from it and recording only the color of the ball. Tou replace the ball back in the urn once you have recorded the color. You then either pick a ball from the same urn (with probability 0.5) or you randomly pick one of the other urns, and draw a ball from that urn. Again, you only record the color of the ball that was picked. Continuing this process is a hidden Markov process. The underlying Markov process has states 1-3 and transition matrix
$$
T = \begin{pmatrix}
.5&.25&.25\\
.25&.5&.25\\
.25&.25&.5
\end{pmatrix}
$$
The emission probabilities are given by the number of red and green balls in each urn. If, for example. urn $i$ contains $i + 2$ red balls and $i + 5$ green balls, we have $P(Y = \mathrm{red}|X = 1) = 3/9$, $P(Y = \mathrm{green}|X = 1) = 6/9$, $P(Y = \mathrm{red}|X = 2) = 4/11$, $P(Y = \mathrm{red}|X = 1) = 7/11$, $P(Y = \mathrm{red}|X = 3) = 5/13$, $P(Y = \mathrm{red}|X =3) = 8/13$.If we observe a sequence of balls, we cannot be sure which state the balls came from, but we can work out various probabilities associated with the process, which is the goal of this chapter.

- Speaking. The key to making sense of this as a hidden Markov model is to transform the sound signal into a sequence of outputs. We model that there are, say, 100 different sounds that people make when talking. We take samples of people speaking and break up the samples into many small sound intervals. We cluster the small sound intervals into 100 clusters - these are the possible outputs of the hidden Markov model. When a person says a word, the state that they are in is that they are saying some word (which we don't directly observe, we only observe the sound wave). We split the signal into small pieces and classify each piece as one of the 100 possible sounds. We observe this sequence of sounds, and we are interested in recovering the probabilities that the person was saying various words. A lot of detail would need to be filled in for you to actually be able to do this!
:::

## Simulation

Let's simulate a hidden Markov model. We return to  @exm-hmm, and create a sample of length 100 from this HMM.

```{r}
library(markovchain)
tmat <- matrix(c(.5, .25, .25, .25, .5, .25, .25, .25, .5), nrow = 3)
mchain <- new("markovchain", trans = tmat, states = as.character(1:3))
unobserved_states <- as.integer(rmarkovchain(100, mchain))

emission_probs <- matrix(c(3, 6, 4, 7, 5, 12), ncol = 2, byrow = T)
emission_probs <- emission_probs/rowSums(emission_probs) #normalize the rows

sapply(1:100, function(x) {
  sample(c("red", "green"), size = 1, replace = T, prob = emission_probs[unobserved_states[x], ])
})
```

::: callout-tryit
Suppose we have a second HMM, similar to the one we just discussed, but there are only two urns. The transition matrix is 
$$
T = \begin{pmatrix}
.9&.1\\
.1&.9
\end{pmatrix}
$$
The emissions probabilities are  $P(Y = \mathrm{red}|X = 1) = .05$,  $P(Y = \mathrm{green}|X = 1) = .95$,  $P(Y = \mathrm{red}|X = 2) = .5$,  $P(Y = \mathrm{red}|X = 2) = .5$.

- Simulate 100 observations from this new HMM.
- Suppose you observe the three sequences given below. You have to decide whether they were generated from the original process, the modified one we just did, or neither. You can download the data set [here](https://raw.githubusercontent.com/speegled/stochastic/main/hmm.csv)

```{r echo=F}
set.seed(123)
tmat2 <- matrix(c(.9, .1, .1, .9), nrow = 2)
mchain2 <- new("markovchain", trans = tmat2, states = as.character(1:2))
unobserved_states2 <- as.integer(rmarkovchain(100, mchain2))

emission_probs2 <- matrix(c(1, 19, 1, 1), ncol = 2, byrow = T)
emission_probs2 <- emission_probs2/rowSums(emission_probs2) #normalize the rows

a1 <- sapply(1:100, function(x) {
  sample(c("red", "green"), size = 1, replace = T, prob = emission_probs2[unobserved_states2[x], ])
})

a2 <- sapply(1:100, function(x) {
  sample(c("red", "green"), size = 1, replace = T, prob = emission_probs[unobserved_states[x], ])
})

a3 <- sample(c("red", "green"), size = 100, replace = T, prob = c(2,1))

list(a1, a2, a3)[sample(1:3)]
```
:::





[^wiki]: This definition comes from [Wikipedia](https://en.wikipedia.org/wiki/Hidden_Markov_model)